{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MgrtQvpomigf"
   },
   "source": [
    "# Вебинар 8. Итоговый проект"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "icPrG1hrmigi"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4JlONj2qmigj"
   },
   "source": [
    "## Recap по финальному проекту"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "veqYxZOmmigj"
   },
   "source": [
    "**Основное**\n",
    "- Целевая метрика precision@5\n",
    "- Бейзлайн решения - [MainRecommender](https://github.com/geangohn/recsys-tutorial/blob/master/src/recommenders.py)\n",
    "- Сдаем ссылку на github с решением. На github должен быть файл recommendations.csv (user_id | [rec_1, rec_2, ...] с рекомендациями. rec_i - реальные id item-ов (из retail_train.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Внимание! При implicit==0.4.4 всё работает верно. \n",
    "# Но на более поздних версиях код может выдавать Segmentation Fault\n",
    "# pip install implicit==0.4.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dciZXW4xgn-d",
    "outputId": "69f72955-636b-4601-a289-521490472589"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decreased # items from 77059 to 5001\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm\n",
    "import re\n",
    "import warnings\n",
    "from random import shuffle, choice\n",
    "from lightgbm import LGBMClassifier\n",
    "from pandarallel import pandarallel\n",
    "from pycaret.classification import ClassificationExperiment\n",
    "from pycaret.classification import load_model, predict_model\n",
    "\n",
    "from src.tools import paralell_execution, ar_split_eq_cpu\n",
    "from src.metrics import money_precision_at_k\n",
    "from src.recommenders import MainRecommender\n",
    "from src.utils import (reduce_mem_usage, prefilter_items, add_features,\n",
    "                       get_new_user_features, get_new_item_features, \n",
    "                       get_important_features, get_final_recs, \n",
    "                       get_popularity_recommendations, get_targets, \n",
    "                       postfilter_items, cat_filter)\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pandarallel.initialize(use_memory_fs=False, verbose=0)\n",
    "\n",
    "data_main = pd.read_csv('data/retail_train.csv')\n",
    "data = data_main[:int(data_main.shape[0]*0.8)]\n",
    "data_test = data_main[int(data_main.shape[0]*0.8):]\n",
    "item_features = pd.read_csv('data/product.csv')\n",
    "user_features = pd.read_csv('data/hh_demographic.csv')\n",
    "\n",
    "# column processing\n",
    "item_features.columns = [col.lower() for col in item_features.columns]\n",
    "item_features.rename(columns={'product_id': 'item_id'}, inplace=True)\n",
    "\n",
    "user_features.columns = [col.lower() for col in user_features.columns]\n",
    "user_features.rename(columns={'household_key': 'user_id'}, inplace=True)\n",
    "\n",
    "N = 150 # 150 кол-во рекомендаций\n",
    "VAL_SIZE = 5 # кол-во недель в валидационной выборке\n",
    "\n",
    "data_train = data[data['week_no'] < data['week_no'].max() - (VAL_SIZE)]\n",
    "val = data[data['week_no'] >= data['week_no'].max() - (VAL_SIZE)]\n",
    "data_train_val = val.copy()\n",
    "\n",
    "# Отбираем подходящие товары\n",
    "n_items_before = data_train['item_id'].nunique()\n",
    "data_train = prefilter_items(data_train, item_features=item_features, take_n_popular=5000)\n",
    "n_items_after = data_train['item_id'].nunique()\n",
    "\n",
    "print(f'Decreased # items from {n_items_before} to {n_items_after}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Всего тестируется параметров: 27\n",
      "**************************************************\n",
      "**************************************************\n",
      "Лучшая метрика money_precision: 0.44947690749871416 при параметрах {'n_factors': 25, 'iterations': 15, 'regularization': 0.05}\n"
     ]
    }
   ],
   "source": [
    "params = []\n",
    "\n",
    "for n_factors in [25, 50, 100]:\n",
    "    for iterations in [15, 25, 35]:\n",
    "        for regularization in [0.001, 0.01, 0.05]:\n",
    "            params.append({\n",
    "                           'n_factors': n_factors, \n",
    "                           'iterations': iterations, \n",
    "                           'regularization': regularization, \n",
    "                          })\n",
    "shuffle(params)\n",
    "print(f'Всего тестируется параметров: {len(params)}')\n",
    "print('*' * 50)\n",
    "\n",
    "def train(params):\n",
    "    p = params['PARAMS']\n",
    "    param = p['main_recommender_params']\n",
    "    data_train_val = p['data_train_val']\n",
    "    data_train = p['data_train']\n",
    "    item_features = p['item_features']\n",
    "    user_features = p['user_features']\n",
    "    N = p['N']\n",
    "    \n",
    "    # учим модель первого уровня\n",
    "    recommender = MainRecommender(data_train, \n",
    "                                  weighting=True, \n",
    "                                  n_factors=param['n_factors'], \n",
    "                                  regularization=param['regularization'], \n",
    "                                  iterations=param['iterations'])\n",
    "    \n",
    "    # делаем эмбеддинги\n",
    "    items_embeddings_df = recommender.items_embeddings_df\n",
    "    users_embeddings_df = recommender.users_embeddings_df\n",
    "    \n",
    "    # добавляем новые фитчи\n",
    "    df_train = add_features(data_train_val,\n",
    "                          data_train,\n",
    "                          recommender,\n",
    "                          item_features, \n",
    "                          user_features,\n",
    "                          items_embeddings_df,\n",
    "                          users_embeddings_df,\n",
    "                          N)\n",
    "    df_test = add_features(data_test, \n",
    "                           data_train, \n",
    "                           recommender, \n",
    "                           item_features,   \n",
    "                           user_features, \n",
    "                           items_embeddings_df, \n",
    "                           users_embeddings_df, \n",
    "                           N)\n",
    "    \n",
    "    # фильтруем название столбцов\n",
    "    df_train = df_train.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
    "    df_test = df_test.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
    "\n",
    "    # сжимаем данные для ускорения\n",
    "    df_test = reduce_mem_usage(df_test)\n",
    "    df_train = reduce_mem_usage(df_train)\n",
    "    \n",
    "    # категориальные переменные\n",
    "    cat_feats = list(set(df_train.columns) - set(df_train._get_numeric_data().columns)) + ['user_id', 'item_id']\n",
    "    # числовые переменные\n",
    "    non_cat_feats = [col for col in df_train if col not in cat_feats]\n",
    "    \n",
    "    # Готовим тренировочную выборку\n",
    "    # готовимся к машинному обучению\n",
    "    X_train = df_train.drop(['target'], axis=1)\n",
    "    y_train = df_train[['target']]\n",
    "    \n",
    "    # категориальные переменные\n",
    "    X_train[cat_feats] = X_train[cat_feats].astype('category')\n",
    "    \n",
    "    # Готовим тестовую выборку\n",
    "    X_test = df_test.drop(['target'], axis=1)\n",
    "    y_test = df_test[['target']]\n",
    "    X_test[cat_feats] = X_test[cat_feats].astype('category')\n",
    "    \n",
    "    # обучаем модель с учётом важности фитч\n",
    "    model = LGBMClassifier(\n",
    "        objective='binary', \n",
    "        max_depth=5, \n",
    "        categorical_column=cat_feats\n",
    "    )\n",
    "    important_feats = get_important_features(model, X_train, y_train)\n",
    "    model.fit(X_train[important_feats], y_train)\n",
    "    \n",
    "    # Делаем предсказания по гоотвой модели\n",
    "    test_preds_proba = model.predict_proba(X_test[important_feats])[:, 1]\n",
    "    \n",
    "    res = get_final_recs(X_test, test_preds_proba, data, data_train, item_features)\n",
    "        \n",
    "    price = data_train.groupby('item_id')['price'].mean().reset_index()\n",
    "    \n",
    "    money_result = res.parallel_apply(lambda row: money_precision_at_k(row['recommendations'], \n",
    "                                                              row['actual'], \n",
    "                                                              price), axis=1).mean()\n",
    "    \n",
    "    return {'money_result': money_result,\n",
    "            'params': param,\n",
    "            'model': model,\n",
    "            'res': res}\n",
    "\n",
    "args_parallel = []\n",
    "for param in ar_split_eq_cpu(params):\n",
    "    args_parallel.append({\n",
    "        'PARAMS':{\n",
    "            'main_recommender_params': param[0],\n",
    "            'data_train_val': data_train_val,\n",
    "            'data_train': data_train,\n",
    "            'item_features': item_features,\n",
    "            'user_features': user_features,\n",
    "            'N': N,\n",
    "        }\n",
    "    })\n",
    "\n",
    "# запускаем процесс параллельных вычислений\n",
    "res = paralell_execution(func=train,\n",
    "                         arg=args_parallel,\n",
    "                         method='multiprocessing')\n",
    "\n",
    "best_money_result = 0.0\n",
    "best_params = {}\n",
    "best_model = None\n",
    "best_res = None\n",
    "\n",
    "for row in res: # объединяем результаты со всех ядер\n",
    "    if row['money_result'] > best_money_result:\n",
    "        best_money_result = row['money_result']\n",
    "        best_params = row['params']\n",
    "        best_model = row['model']\n",
    "        best_res = row['res']\n",
    "\n",
    "print('*' * 50)\n",
    "print(f'Лучшая метрика money_precision: {best_money_result} '+\\\n",
    "      f'при параметрах {best_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "lfNVN4_8j7gd"
   },
   "outputs": [],
   "source": [
    "# Сохранение самой успешной модели\n",
    "best_res.drop('actual', axis=1, inplace=True)\n",
    "\n",
    "best_res.head(3)\n",
    "\n",
    "best_res.to_csv('recommendations.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "webinar_8.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
